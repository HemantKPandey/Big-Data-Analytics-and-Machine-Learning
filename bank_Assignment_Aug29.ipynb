{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Excercise to Understand Basic data Processing Using Spark and getting readyfor using Spark for Machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0.0 Calling Requried Libraries to make things smoother\n",
    "from IPython.core.interactiveshell import InteractiveShell # to make better interaction with shell\n",
    "from IPython.core.display import display, HTML # to display output as html to avoid truncation or mis reading\n",
    "from pyspark.sql.functions import col,sum # col() returns a Column (values) based on a given column name. sum will be used as normal sum function.\n",
    "from pyspark.sql.types import DoubleType #  Import class 'DoubleType' that represents double data type.\n",
    "from pyspark.ml.feature import StringIndexer # For Processing Categorical String Variables\n",
    "from pyspark.ml.feature import OneHotEncoderEstimator # for preparing Categorical String Indexer output columns for Machine learning and analysis\n",
    "from pyspark.ml import Pipeline # to create Pipeline to perform multiple Stages in one go.\n",
    "from pyspark.ml.feature import VectorAssembler # to create One feature columns assembled by adding the Multiple columns of dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Formatting The Spark and Verifying Spark instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1.1 Display multiple outputs from a cell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#1.2 Increase cell width to display wide columnar output\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.56.101:4042\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=PySparkShell>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.56.101:4042\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f485d9b7b70>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1.3 To check if spark context is ,sc, and spark session is, spark: \n",
    "#        try the following two commands. Also right-click on Spark_UI hyperlink\n",
    "#        to open another site.\n",
    "#      Spark 'driver' process manifests itself through 'spark' session object\n",
    "\n",
    "sc\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data was moved to Hadoop File system using below commands ( Assignemnt Q.1 )\n",
    "        ''' \n",
    "        cd ~\n",
    "         hdfs dfs -put /home/ashok/Excercise/bank.csv hdfs://localhost:9000/user/ashok/data_files/bank.csv\n",
    "         hdfs dfs -ls -h hdfs://localhost:9000/user/ashok/data_files\n",
    "        '''\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading Dataset from Hadoop file system. ( Assignment Q.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Read, transform and understand the data\n",
    "#      https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrameReader.csv\n",
    "#    pyspark creates a spark-session variable: spark\n",
    "df = spark.read.csv(\n",
    "                   path = \"hdfs://localhost:9000/user/ashok/data_files/bank.csv\",   # path to hadoop\n",
    "                   header = True,\n",
    "                   inferSchema= True,           # Infer datatypes automatically\n",
    "                   sep = \",\" ,                  # Can be any character (check \\t)\n",
    "                   )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating Cache of the dataframe and Looking at verious details of Data frame \"df\" ( Assignment Q.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[age: int, job: string, marital: string, education: string, default: string, balance: int, housing: string, loan: string, contact: string, day: int, month: string, duration: int, campaign: int, pdays: int, previous: int, poutcome: string, deposit: string]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3.1 We also cache the data so that we only read it from disk once.\n",
    "df.cache()\n",
    "df.is_cached            # Checks if df is cached"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11162, 17)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "11162"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3.2 Size of data df\n",
    "df.count(), len(df.columns)\n",
    "# Number of Rows\n",
    "df.count()\n",
    "# Number of Columns\n",
    "len(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+-------+---------+-------+-------+-------+----+-------+---+-----+--------+--------+-----+--------+--------+-------+\n",
      "|age|       job|marital|education|default|balance|housing|loan|contact|day|month|duration|campaign|pdays|previous|poutcome|deposit|\n",
      "+---+----------+-------+---------+-------+-------+-------+----+-------+---+-----+--------+--------+-----+--------+--------+-------+\n",
      "| 59|    admin.|married|secondary|     no|   2343|    yes|  no|unknown|  5|  may|    1042|       1|   -1|       0| unknown|    yes|\n",
      "| 56|    admin.|married|secondary|     no|     45|     no|  no|unknown|  5|  may|    1467|       1|   -1|       0| unknown|    yes|\n",
      "| 41|technician|married|secondary|     no|   1270|    yes|  no|unknown|  5|  may|    1389|       1|   -1|       0| unknown|    yes|\n",
      "| 55|  services|married|secondary|     no|   2476|    yes|  no|unknown|  5|  may|     579|       1|   -1|       0| unknown|    yes|\n",
      "| 54|    admin.|married| tertiary|     no|    184|     no|  no|unknown|  5|  may|     673|       2|   -1|       0| unknown|    yes|\n",
      "+---+----------+-------+---------+-------+-------+-------+----+-------+---+-----+--------+--------+-----+--------+--------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 3.3 Show data. Note that \n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: integer (nullable = true)\n",
      " |-- job: string (nullable = true)\n",
      " |-- marital: string (nullable = true)\n",
      " |-- education: string (nullable = true)\n",
      " |-- default: string (nullable = true)\n",
      " |-- balance: integer (nullable = true)\n",
      " |-- housing: string (nullable = true)\n",
      " |-- loan: string (nullable = true)\n",
      " |-- contact: string (nullable = true)\n",
      " |-- day: integer (nullable = true)\n",
      " |-- month: string (nullable = true)\n",
      " |-- duration: integer (nullable = true)\n",
      " |-- campaign: integer (nullable = true)\n",
      " |-- pdays: integer (nullable = true)\n",
      " |-- previous: integer (nullable = true)\n",
      " |-- poutcome: string (nullable = true)\n",
      " |-- deposit: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('age', 'int'),\n",
       " ('job', 'string'),\n",
       " ('marital', 'string'),\n",
       " ('education', 'string'),\n",
       " ('default', 'string'),\n",
       " ('balance', 'int'),\n",
       " ('housing', 'string'),\n",
       " ('loan', 'string'),\n",
       " ('contact', 'string'),\n",
       " ('day', 'int'),\n",
       " ('month', 'string'),\n",
       " ('duration', 'int'),\n",
       " ('campaign', 'int'),\n",
       " ('pdays', 'int'),\n",
       " ('previous', 'int'),\n",
       " ('poutcome', 'string'),\n",
       " ('deposit', 'string')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3.4 Have a look at the dataframe schema,i.e. the structure of the DataFrame\n",
    "   \n",
    "df.printSchema()\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-------+---------+-------+-------+-------+----+-------+---+-----+--------+--------+-----+--------+--------+-------+\n",
      "|age|job|marital|education|default|balance|housing|loan|contact|day|month|duration|campaign|pdays|previous|poutcome|deposit|\n",
      "+---+---+-------+---------+-------+-------+-------+----+-------+---+-----+--------+--------+-----+--------+--------+-------+\n",
      "|  0|  0|      0|        0|      0|      0|      0|   0|      0|  0|    0|       0|       0|    0|       0|       0|      0|\n",
      "+---+---+-------+---------+-------+-------+-------+----+-------+---+-----+--------+--------+-----+--------+--------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 3.5 Check missing values:\n",
    "df.select(*(sum(col(c).isNull().cast(\"int\")).alias(c) for c in df.columns)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Processing the data in \"df\" to work on required Data, dropping unnecessary columns. (Assignment Q.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+-------+---------+-------+-------+-------+----+-------+--------+--------+-----+--------+--------+-------+\n",
      "|age|       job|marital|education|default|balance|housing|loan|contact|duration|campaign|pdays|previous|poutcome|deposit|\n",
      "+---+----------+-------+---------+-------+-------+-------+----+-------+--------+--------+-----+--------+--------+-------+\n",
      "| 59|    admin.|married|secondary|     no|   2343|    yes|  no|unknown|    1042|       1|   -1|       0| unknown|    yes|\n",
      "| 56|    admin.|married|secondary|     no|     45|     no|  no|unknown|    1467|       1|   -1|       0| unknown|    yes|\n",
      "| 41|technician|married|secondary|     no|   1270|    yes|  no|unknown|    1389|       1|   -1|       0| unknown|    yes|\n",
      "+---+----------+-------+---------+-------+-------+-------+----+-------+--------+--------+-----+--------+--------+-------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 4.0 To select Data only from required columns i.e.:['age', 'job', 'marital', 'education', 'default',  'balance','housing', 'loan', 'contact',\n",
    "# 'duration',  'campaign', 'pdays', 'previous', 'poutcome', 'deposit']\n",
    "# drop individual columns to get required columns. as per assignment.\n",
    "\n",
    "df = df.drop(\"day\").drop(\"month\")\n",
    "df.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Processing String Categorical Columns using StringIndexer and OneHotEncoderEstimator. Then creating Stages for using in Pipeline (Assignment Q.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['age',\n",
       " 'job',\n",
       " 'marital',\n",
       " 'education',\n",
       " 'default',\n",
       " 'balance',\n",
       " 'housing',\n",
       " 'loan',\n",
       " 'contact',\n",
       " 'duration',\n",
       " 'campaign',\n",
       " 'pdays',\n",
       " 'previous',\n",
       " 'poutcome',\n",
       " 'deposit',\n",
       " 'stringindexed_job',\n",
       " 'stringindexed_marital',\n",
       " 'stringindexed_education',\n",
       " 'stringindexed_default',\n",
       " 'stringindexed_housing',\n",
       " 'stringindexed_loan',\n",
       " 'stringindexed_contact',\n",
       " 'stringindexed_poutcome',\n",
       " 'onehotencoded_default',\n",
       " 'onehotencoded_contact',\n",
       " 'onehotencoded_education',\n",
       " 'onehotencoded_housing',\n",
       " 'onehotencoded_marital',\n",
       " 'onehotencoded_loan',\n",
       " 'onehotencoded_poutcome',\n",
       " 'onehotencoded_job']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# v) The list of categorical columns is as follows:\n",
    "\n",
    "catCols = ['job', 'marital', 'education', 'default', 'housing', 'loan', 'contact', 'poutcome']\n",
    "\n",
    "# Create a pipeline to transform each one of the categorical columns to as many Onehotencoded columns\t\n",
    "# by first using  StringIndexer and then OneHotEncoder. \n",
    "\n",
    "# 5 ======= build stages ================\n",
    "# Creating StringIndexer Stage for all the \n",
    "stringindexer_stages = [StringIndexer(inputCol=c, outputCol='stringindexed_' + c) for c in catCols]\n",
    "# Creating StringIndexer Stage for all the\n",
    "in_cols = ['stringindexed_' + c for c in catCols]\n",
    "out_cols = ['onehotencoded_' + c  for c in catCols]\n",
    "onehotencoder_stages = [OneHotEncoderEstimator(inputCols=in_cols, outputCols=out_cols)]\n",
    "\n",
    "#  Create Pipeline \n",
    "pipeline = Pipeline(stages=stringindexer_stages+onehotencoder_stages)\n",
    "\n",
    "##  Fit pipeline model\n",
    "pipeline_mode = pipeline.fit(df)\n",
    "\n",
    "## Transform data\n",
    "df_coded = pipeline_mode.transform(df)\n",
    "df_coded.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using Vector Assembler to use OneHotEncoded Columns and Numeric Columns for Assembling featured columns. ( Assignment Q.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('age', 'double'),\n",
       " ('job', 'string'),\n",
       " ('marital', 'string'),\n",
       " ('education', 'string'),\n",
       " ('default', 'string'),\n",
       " ('balance', 'double'),\n",
       " ('housing', 'string'),\n",
       " ('loan', 'string'),\n",
       " ('contact', 'string'),\n",
       " ('duration', 'double'),\n",
       " ('campaign', 'double'),\n",
       " ('pdays', 'double'),\n",
       " ('previous', 'double'),\n",
       " ('poutcome', 'string'),\n",
       " ('deposit', 'string'),\n",
       " ('stringindexed_job', 'double'),\n",
       " ('stringindexed_marital', 'double'),\n",
       " ('stringindexed_education', 'double'),\n",
       " ('stringindexed_default', 'double'),\n",
       " ('stringindexed_housing', 'double'),\n",
       " ('stringindexed_loan', 'double'),\n",
       " ('stringindexed_contact', 'double'),\n",
       " ('stringindexed_poutcome', 'double'),\n",
       " ('onehotencoded_default', 'vector'),\n",
       " ('onehotencoded_contact', 'vector'),\n",
       " ('onehotencoded_education', 'vector'),\n",
       " ('onehotencoded_housing', 'vector'),\n",
       " ('onehotencoded_marital', 'vector'),\n",
       " ('onehotencoded_loan', 'vector'),\n",
       " ('onehotencoded_poutcome', 'vector'),\n",
       " ('onehotencoded_job', 'vector')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# vi) Use VectorAssembler to aseemble all the OneHotEncoded columns and the following numerical columns in one column.\n",
    "# Call this new assembled column as: 'rawFeatures' :\n",
    "\n",
    "numericCols = ['age', 'balance', 'duration',  'campaign', 'pdays', 'previous']\n",
    "\n",
    "# 6.1 We will now cast the rest of  columns into doubletype in one go as VectorAssembler will need that kind of data for Processing and making it fearured column\n",
    "for l in numericCols:\n",
    "    df_coded = df_coded.withColumn(l, col(l).cast(DoubleType()))\n",
    "    \n",
    "df_coded.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['age',\n",
       " 'balance',\n",
       " 'duration',\n",
       " 'campaign',\n",
       " 'pdays',\n",
       " 'previous',\n",
       " 'onehotencoded_job',\n",
       " 'onehotencoded_marital',\n",
       " 'onehotencoded_education',\n",
       " 'onehotencoded_default',\n",
       " 'onehotencoded_housing',\n",
       " 'onehotencoded_loan',\n",
       " 'onehotencoded_contact',\n",
       " 'onehotencoded_poutcome']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 6.2 Which columns to concatenate: as per Question in Assignment : the OneHotEncoded columns and the following numerical columns in one column.\n",
    "Feature_columns= numericCols+out_cols\n",
    "# Now we will see clumn names, that will be helpful as featuresd single Indexed column.\n",
    "Feature_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.3   Create an instance of VectorAssembler class.\n",
    "#          This object will be used to transfrom data farme,\n",
    "#           as: vectorassembler.fit(df)\n",
    "vectorassembler = VectorAssembler(\n",
    "                                  inputCols=Feature_columns,\n",
    "                                  outputCol=\"rawFeatures\"\n",
    "                                 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Printing Output.rawFeatures Column to verify what all are included in raw features column. (Assignment Q.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------------------------------------------------------------+\n",
      "|rawFeatures                                                                                        |\n",
      "+---------------------------------------------------------------------------------------------------+\n",
      "|(30,[0,1,2,3,4,9,17,19,22,24,26,27],[59.0,2343.0,1042.0,1.0,-1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])     |\n",
      "|(30,[0,1,2,3,4,9,17,19,22,23,24,26,27],[56.0,45.0,1467.0,1.0,-1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])|\n",
      "|(30,[0,1,2,3,4,8,17,19,22,24,26,27],[41.0,1270.0,1389.0,1.0,-1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])     |\n",
      "+---------------------------------------------------------------------------------------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Now Assembling columns in Vector Assembler on NUmeric Col and Onehotencoded columns\n",
    "output = vectorassembler.transform(df_coded)\n",
    "# Just see the 'rawFeatures' column\n",
    "output.select('rawFeatures').show(3,truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
